<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Hadoop | Zhichao's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-158590818-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'c1bb31022c448b72f7350c6d43905276';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
  </script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
  })();</script><meta name="generator" content="Hexo 4.2.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Hadoop</h1><a id="logo" href="/.">Zhichao's Blog</a><p class="description">Zhichao Jin</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/guestbook/"><i class="fa fa-comments"> 留言</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Hadoop</h1><div class="post-meta">Jul 3, 2022<span> | </span><span class="category"><a href="/categories/Linux/">Linux</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 10</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2022/07/03/linux/hadoop-spark-tutorial/#vcomment"><span class="valine-comment-count" data-xid="/2022/07/03/linux/hadoop-spark-tutorial/"></span><span> 条评论</span></a><div class="post-content"><blockquote>
<p>[1] <a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Hadoop集群环境搭建.md" target="_blank" rel="noopener">BigData-Notes/Hadoop集群环境搭建.md at master · heibaiying/BigData-Notes (github.com)</a></p>
<p>[2] <a href="https://www.jianshu.com/p/f83229af1898" target="_blank" rel="noopener">Hadoop集群配置 - 简书 (jianshu.com)</a></p>
</blockquote>
<p>安装方法参考官网 <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Required_Software" target="_blank" rel="noopener">Apache Hadoop 3.3.3 – Hadoop: Setting up a Single Node Cluster.</a></p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><blockquote>
<p><a href="http://c.biancheng.net/view/6502.html" target="_blank" rel="noopener">HDFS简明入门教程 (biancheng.net)</a></p>
</blockquote>
<p>操作步骤如下：</p>
<ol>
<li>客户端发起文件读取的请求。</li>
<li>NameNode 将文件对应的数据块信息及每个块的位置信息，包括每个块的所有副本的位置信息（即每个副本所在的 DataNode 的地址信息）都传送给客户端。</li>
<li>客户端收到数据块信息后，直接和数据块所在的 DataNode 通信，并行地读取数据块。</li>
</ol>
<p>在客户端获得 NameNode 关于每个数据块的信息后，客户端会根据网络拓扑选择与它最近的 DataNode 来读取每个数据块。当与 DataNode 通信失败时，它会选取另一个较近的 DataNode，同时会对出故障的 DataNode 做标记，避免与它重复通信，并发送 NameNode 故障节点的信息。</p>
<h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><p>设置环境变量：</p>
<ol>
<li><code>JAVA_HOME={path_to_jdk_dir}</code></li>
<li><code>HADOOP_HOME={path_to_hadoop_home}</code></li>
</ol>
<p>多台设备间能免密<code>ssh</code>：</p>
<ol>
<li>修改每台服务器的<strong>hostname</strong></li>
<li>:star:在<code>\etc\hosts</code>中绑定集群中每个设备的IP地址与域名（使用真实IP表示本机而不用localhost）</li>
<li>在<code>~/.ssh</code>生成公密钥，用<code>ssh-copy-id</code>命令将公钥添加到其他设备</li>
<li>使用<code>ssh {hostname}</code>命令验证是否能免密登录其他主机</li>
</ol>
<h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><p>修改<code>hadoop\etc</code>中的配置文件，如下。</p>
<h3 id="1-core-site-xml"><a href="#1-core-site-xml" class="headerlink" title="1. core-site.xml"></a>1. core-site.xml</h3><blockquote>
<p>The <code>fs.defaultFS</code> makes HDFS a file abstraction over a cluster, so that its root is not the same as the local system’s.</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 通过该参数配置集群的文件抽象 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>131072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-hdfs-site-xml"><a href="#2-hdfs-site-xml" class="headerlink" title="2. hdfs-site.xml"></a>2. hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/namenode/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 节点数据（即数据块）的存放位置 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/datanode/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-hadoop-env-sh"><a href="#3-hadoop-env-sh" class="headerlink" title="3. hadoop-env.sh"></a>3. hadoop-env.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export PDSH_RCMD_TYPE=ssh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> HDFS_NAMENODE_USER=hadoop</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> HDFS_DATANODE_USER=hadoop</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=hadoop</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=hadoop</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">export</span> YARN_NODEMANAGER_USER=hadoop</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="4-yarn-site-xml-可选，如果不使用分布式计算"><a href="#4-yarn-site-xml-可选，如果不使用分布式计算" class="headerlink" title="4. yarn-site.xml (可选，如果不使用分布式计算)"></a>4. yarn-site.xml (可选，如果不使用分布式计算)</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-mapred-site-xml-可选，如果不使用MapReduce"><a href="#5-mapred-site-xml-可选，如果不使用MapReduce" class="headerlink" title="5. mapred-site.xml (可选，如果不使用MapReduce)"></a>5. mapred-site.xml (可选，如果不使用MapReduce)</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 mapreduce 作业运行在 yarn 上--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="6-wokers"><a href="#6-wokers" class="headerlink" title="6. wokers"></a>6. wokers</h3><p>在目录的<code>wokers</code>中添加所有的datanode节点的主机名</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop-<span class="literal">master</span></span><br><span class="line">hadoop-slave1</span><br><span class="line">hadoop-slave2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>修改配置文件后，在namenode节点（即master）使用<code>hdfs namenode -format</code>格式化namenode的文件系统（不需要在slave节点上执行该命令）</p>
<p>执行<code>sbin/start-all.sh</code>命令开启服务。</p>
<p>由于是集群，因此任务的执行会在所有的设备上触发。</p>
<blockquote>
<p>Typically one machine in the cluster is designated as the <strong>NameNode</strong> and another machine as the <strong>ResourceManager</strong>, <strong>exclusively</strong>. <strong>These are the masters.</strong> Other services (such as Web App Proxy Server and MapReduce Job History server) are usually run either on dedicated hardware or on shared infrastructure, depending upon the load.</p>
<p>The rest of the machines in the cluster act as both <strong>DataNode</strong> and <strong>NodeManager</strong>. <strong>These are the workers.</strong></p>
</blockquote>
<blockquote>
<p>[hadoop slaves_猎人在吃肉的博客-CSDN博客](<a href="https://blog.csdn.net/xiaojin21cen/article/details/42421781" target="_blank" rel="noopener">https://blog.csdn.net/xiaojin21cen/article/details/42421781</a></p>
</blockquote>
<p><strong>slaves 文件 (在新版中是worker文件)</strong></p>
<p>一般在集群中你唯一地选择一台机器作为 NameNode ，一台机器作为 ResourceManager，这是master  (主)。</p>
<p>那么，集群中剩下的机器作为DataNode 和 NodeManager。这些是slaves(从)。</p>
<p>在你的<a href="https://so.csdn.net/so/search?q=hadoop&spm=1001.2101.3001.7020" target="_blank" rel="noopener">hadoop</a>目录/etc/hadoop/slaves文件上列出全部slave机器名或IP地址，一个一行。</p>
<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><blockquote>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#linking-with-spark" target="_blank" rel="noopener">RDD Programming Guide - Spark 3.2.1 Documentation (apache.org)</a></p>
</blockquote>
<p>To run Spark applications in Python without pip installing PySpark, use the <code>bin/spark-submit</code> script located in the Spark directory. This script will load Spark’s Java/Scala libraries and allow you to submit applications to a <code>cluster</code>. You can also use <code>bin/pyspark</code> to launch an interactive Python shell.</p>
<p>使用spark-submit提交应用到集群，连接集群需要初始化</p>
<h3 id="集群连接方法"><a href="#集群连接方法" class="headerlink" title="集群连接方法"></a>集群连接方法</h3><blockquote>
<ol>
<li><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#initializing-spark" target="_blank" rel="noopener">initializing-spark</a></li>
<li><a href="https://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit" target="_blank" rel="noopener">spark-submit</a></li>
</ol>
</blockquote>
<p>The first thing a Spark program must do is to create a <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html#pyspark.SparkContext" target="_blank" rel="noopener">SparkContext</a> object, which tells Spark how to access a cluster. To create a <code>SparkContext</code> you first need to build a <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html#pyspark.SparkConf" target="_blank" rel="noopener">SparkConf</a> object that contains information about your application.</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = <span class="constructor">SparkConf()</span>.set<span class="constructor">AppName(<span class="params">appName</span>)</span>.set<span class="constructor">Master(<span class="params">master</span>)</span></span><br><span class="line">sc = <span class="constructor">SparkContext(<span class="params">conf</span>=<span class="params">conf</span>)</span></span><br></pre></td></tr></table></figure>

<p>The <code>appName</code> parameter is a name for your application to show on the cluster UI. <code>master</code> is a <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="noopener">Spark, Mesos or YARN cluster URL</a>, or a special “local” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode <code>master</code> in the program, but rather <a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">launch the application with <code>spark-submit</code></a> and receive it there. However, for local testing and unit tests, you can pass “local” to run Spark in-process.</p>
<p>连接集群需要创建SparkContext对象，描述集群的连接方法，使用配置文件传入应用名与master（可理解集群的类型）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Unlike other cluster managers supported by Spark in which the master’s address is specified in the <code>--master</code> parameter, in YARN mode the ResourceManager’s address is picked up from the Hadoop configuration. Thus, the <code>--master</code> parameter is <code>yarn</code>.</p>
</blockquote>
<p>此处的master如果使用Hadoop Yarn可以传入<code>yarn</code>，而不是<code>spark://...</code>，具体的脚本参数可查看官网。</p>
<p>PySpark applications start with initializing <code>SparkSession</code> which is the entry point of PySpark as below. In case of running it in PySpark shell via pyspark executable, the shell automatically creates the session in the variable spark for users.</p>
<p>通过SparkSession开启Spark应用，交互式的<code>bin/pyspark</code>中已经创建了session。</p>
<h3 id="在集群上运行（Cluster-Manager-Types）"><a href="#在集群上运行（Cluster-Manager-Types）" class="headerlink" title="在集群上运行（Cluster Manager Types）"></a>在集群上运行（Cluster Manager Types）</h3><blockquote>
<p><a href="https://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Running Spark on YARN - Spark 3.2.1 Documentation (apache.org)</a></p>
</blockquote>
<p>The system currently supports several cluster managers:</p>
<ul>
<li><a href="https://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Standalone</a> – a simple cluster manager included with Spark that makes it easy to set up a cluster.</li>
<li><a href="https://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="noopener">Apache Mesos</a> – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)</li>
<li><a href="https://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Hadoop YARN</a> – the resource manager in Hadoop 2.</li>
<li><a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">Kubernetes</a> – an open-source system for automating deployment, scaling, and management of containerized applications.</li>
</ul>
<p>Spark支持四种集群模式。除了本地测试，此处选择在Hadoop Yarn上运行Spark</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_noisy</span><span class="params">(cx, cy, N=<span class="number">50</span>, std=<span class="number">1</span>)</span>:</span></span><br><span class="line">    noise_x = std * np.random.randn(N)</span><br><span class="line">    noise_y = std * np.random.randn(N)</span><br><span class="line"></span><br><span class="line">    X = np.array([cx + noise <span class="keyword">for</span> noise <span class="keyword">in</span> noise_x])</span><br><span class="line">    Y = np.array([cy + noise <span class="keyword">for</span> noise <span class="keyword">in</span> noise_y])</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_dataset</span><span class="params">(centers)</span>:</span></span><br><span class="line">    X = np.zeros([<span class="number">0</span>])</span><br><span class="line">    Y = np.zeros([<span class="number">0</span>])</span><br><span class="line">    labels = np.zeros([<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, center <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">        X_i, Y_i = generate_noisy(*center)</span><br><span class="line">        labels_i = i * np.ones_like(X_i)</span><br><span class="line"></span><br><span class="line">        X = np.concatenate([X, X_i])</span><br><span class="line">        Y = np.concatenate([Y, Y_i])</span><br><span class="line">        labels = np.concatenate([labels, labels_i])</span><br><span class="line">    <span class="keyword">return</span> X.tolist(), Y.tolist(), labels.astype(np.int16).tolist()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eu_distance</span><span class="params">(p1, p2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.square(p1 - p2)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KMeans</span><span class="params">(data, k=<span class="number">2</span>, seed=<span class="number">1</span>, num_iter=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        `data`: 2D points in format (x, y)</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        `centers`: [(cx1, cy1), (cx2, cy2), ... ]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    centers = random.sample(data, k)</span><br><span class="line">    data = np.array(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_iter):</span><br><span class="line">        clusters = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(k)]</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">            dist = [eu_distance(item, np.array(c)) <span class="keyword">for</span> c <span class="keyword">in</span> centers]</span><br><span class="line">            clusters[dist.index(min(dist))].append(item)</span><br><span class="line">        centers = [np.array(c).mean(axis=<span class="number">0</span>).tolist() <span class="keyword">for</span> c <span class="keyword">in</span> clusters]</span><br><span class="line">    points = []</span><br><span class="line">    <span class="keyword">for</span> label, cluster <span class="keyword">in</span> enumerate(clusters):</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> cluster:</span><br><span class="line">            points.append((x, y, label))</span><br><span class="line">    <span class="keyword">return</span> centers, points</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_clustering</span><span class="params">(x, y, labels, centers)</span>:</span></span><br><span class="line"></span><br><span class="line">    colors = [[<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line">    colors_center = [<span class="string">'#000000'</span>, <span class="string">'#66ffff'</span>]</span><br><span class="line"></span><br><span class="line">    f, ax = plt.subplots()</span><br><span class="line">    ax.scatter(x, y, color=colors)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, center <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> center:</span><br><span class="line">            ax.scatter(x, y, color=colors_center[i])</span><br><span class="line"></span><br><span class="line">    ax.set_xlabel(<span class="string">'x'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'y'</span>)</span><br><span class="line">    f.suptitle(<span class="string">'K-Means'</span>)</span><br><span class="line"></span><br><span class="line">    plt.savefig(<span class="string">'./result.jpg'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">"k-means"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    file_path = <span class="string">"hdfs://hadoop-master:8020/user/hadoop/data.csv"</span></span><br><span class="line"></span><br><span class="line">    centers = [(random.randint(<span class="number">1</span>, <span class="number">8</span>), random.randint(<span class="number">1</span>, <span class="number">8</span>)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">    data = generate_dataset(centers)</span><br><span class="line">    spark.createDataFrame(</span><br><span class="line">        list(zip(*data))).write.mode(<span class="string">'overwrite'</span>).csv(file_path)</span><br><span class="line"></span><br><span class="line">    df = spark.read.csv(file_path, inferSchema=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    points = [(row[<span class="number">0</span>], row[<span class="number">1</span>]) <span class="keyword">for</span> row <span class="keyword">in</span> df.collect()]</span><br><span class="line">    centers_hat, points = KMeans(points, k=<span class="number">3</span>, seed=<span class="number">2</span>, num_iter=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"GroundTruth: <span class="subst">&#123;centers&#125;</span>\nPredict: <span class="subst">&#123;centers_hat&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">    x, y, labels = zip(*points)</span><br><span class="line">    plot_clustering(x, y, labels, (centers, centers_hat))</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br><span class="line"><span class="comment"># %%</span></span><br></pre></td></tr></table></figure>

</div><iframe src="/donate/?AliPayQR=/img/alipay_me.png&amp;WeChatQR=/img/wechat_me.png&amp;GitHub=https://github.com/charjindev/donate-page&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Char Jin</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2022/07/03/linux/hadoop-spark-tutorial/">https://blog.charjin.top/2022/07/03/linux/hadoop-spark-tutorial/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://blog.charjin.top/2022/07/03/linux/hadoop-spark-tutorial/" data-id="clhhn3hvl00bptbrlevdfbxo2" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACuUlEQVR42u3a0W7jMAwEwP7/T/eAe3e8S4nXHjB+ChIn1qiAyJL8+oqv779Xe+f3patdQ33h4eHhjZae//TTY56+e/Jp/v7TL+Ph4eHt8fIjPv+0XXS+Zcma8fDw8H4DLwkSyXKfNiVB4uHh4f1fvDa1/QzIA0+b1uPh4eH9G16eBOdH9ixI5AHpcq0FDw8Przul67bWz75e7O/h4eHhHXTV2zQ6KenmYebCwAEeHh7eAq8txeYpct4Yy0vDbfECDw8Pb493fii3yzpJ2dvNwsPDw9vgJYFhVmZNRgQ2gg0eHh7eHi9pGrXjAicl2uJvkuDx8PDwFnjFTfEhnifQeeO/XUkdW/Dw8PBi3uzwzTF5mfV8AqKor+Dh4eEd89o0+nNCnA8KzFL2Ylvx8PDw1nizckPevkpGrGbPfdk+PDw8vAVenljfSpTz4NEGp8dcGg8PD+8qL39kXrDIA0nbDKtLxnh4eHjLvLZw0N4zKxMnoShKpvHw8PAu8dqSRPvgNsnOMUWzDQ8PD+8qr11oXrCYDQG04wgvq8LDw8Nb4O2NVeXl3SRszEoneHh4eBu8Wbrc5uxtufZ82AsPDw9vm3dSZj1vcc3GqqLn4uHh4S3zEuRsO9pxq7ac8fhreHh4eMu8WQsqx7TDWxcuPDw8vGXebASq3YKTcNKm7Hh4eHgbvLasMGuJ5c2w9v5idAAPDw/vEq89uPPSwPkWnKTXL/09PDw8vEu8vHDQFmHb0JKc6nUyjYeHh3eJd1IRPWmbJa9n371W8MXDw8P7yDsJBvlBn6fUSQk4eR8PDw9vm5cvoi0c5M2w2Wa93ImHh4f3o7yiq1Yi8zQ9WQ8eHh7eb+PNDuuTEkPeQnscIMDDw8Nb4CUlhpOC7Gy8IAk/L0/Ew8PDW+DN/tXPhwbyTczD0oWeHh4eHt6E9wfiS7GGF2FFTwAAAABJRU5ErkJggg==">分享</a><div class="tags"></div><div class="post-nav"><a class="pre" href="/2022/07/18/linux/docker-issues/">Issue in Docker</a><a class="next" href="/2022/06/25/linux/wake-on-lan/">Wake on LAN 使用网络唤醒主机</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' ? true : false;
var verify = 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'t4DnKL6Wzc2cJnKlPvgi1mFm-gzGzoHsz',
  appKey:'wRCuQw2SaMNzhBbLJClaYxn4',
  placeholder:'Just leave your comment, Haha...',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">39</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/">开发框架</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a><span class="category-list-count">46</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/">程序设计</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/">编程笔记</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BD%AF%E6%8A%80%E8%83%BD/">软技能</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/" style="font-size: 15px;">树状数组</a> <a href="/tags/%E6%9F%A5%E6%89%BE/" style="font-size: 15px;">查找</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 15px;">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 15px;">二叉树</a> <a href="/tags/%E9%80%92%E5%BD%92%E6%B1%82%E8%A7%A3/" style="font-size: 15px;">递归求解</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/" style="font-size: 15px;">二叉搜索树</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 15px;">动态规划</a> <a href="/tags/%E6%9C%80%E9%95%BF%E4%B8%8D%E4%B8%8B%E9%99%8D%E5%AD%90%E5%BA%8F%E5%88%97/" style="font-size: 15px;">最长不下降子序列</a> <a href="/tags/PAT/" style="font-size: 15px;">PAT</a> <a href="/tags/%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">编程</a> <a href="/tags/%E7%94%9F%E6%B4%BB/" style="font-size: 15px;">生活</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 15px;">并查集</a> <a href="/tags/C-C/" style="font-size: 15px;">C/C++</a> <a href="/tags/OJ/" style="font-size: 15px;">OJ</a> <a href="/tags/%E6%A0%88/" style="font-size: 15px;">栈</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size: 15px;">队列</a> <a href="/tags/%E5%A0%86%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">堆排序</a> <a href="/tags/%E5%A0%86/" style="font-size: 15px;">堆</a> <a href="/tags/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/" style="font-size: 15px;">版本控制</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 15px;">博客</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/%E7%AB%99%E7%82%B9url%E8%87%AA%E5%8A%A8%E6%8E%A8%E9%80%81/" style="font-size: 15px;">站点url自动推送</a> <a href="/tags/Kotlin/" style="font-size: 15px;">Kotlin</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" style="font-size: 15px;">编程语言</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/%E7%B4%A0%E6%95%B0%E5%88%A4%E6%96%AD/" style="font-size: 15px;">素数判断</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 15px;">模拟</a> <a href="/tags/%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2/" style="font-size: 15px;">进制转换</a> <a href="/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" style="font-size: 15px;">排序算法</a> <a href="/tags/%E7%AE%80%E5%8D%95%E6%8E%92%E5%BA%8F/" style="font-size: 15px;">简单排序</a> <a href="/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%AE%97%E6%B3%95/" style="font-size: 15px;">最短路径算法</a> <a href="/tags/Dijkstra%E7%AE%97%E6%B3%95/" style="font-size: 15px;">Dijkstra算法</a> <a href="/tags/%E6%9C%89%E5%BA%8F%E5%BA%8F%E5%88%97%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/" style="font-size: 15px;">有序序列的中位数</a> <a href="/tags/Dijkstra%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%AE%97%E6%B3%95/" style="font-size: 15px;">Dijkstra最短路径算法</a> <a href="/tags/DFS/" style="font-size: 15px;">DFS</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 15px;">深度优先搜索</a> <a href="/tags/%E5%89%AA%E6%9E%9D/" style="font-size: 15px;">剪枝</a> <a href="/tags/BFS/" style="font-size: 15px;">BFS</a> <a href="/tags/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 15px;">广度优先搜索</a> <a href="/tags/%E6%A8%A1%E6%8B%9F%E9%97%AE%E9%A2%98/" style="font-size: 15px;">模拟问题</a> <a href="/tags/%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 15px;">平衡二叉树</a> <a href="/tags/AVL/" style="font-size: 15px;">AVL</a> <a href="/tags/%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 15px;">完全二叉树</a> <a href="/tags/%E5%AE%8C%E5%85%A8%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 15px;">完全平衡二叉树</a> <a href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9A%84%E5%88%A4%E5%88%AB/" style="font-size: 15px;">红黑树的判别</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%80%E4%BD%8E%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/" style="font-size: 15px;">二叉树最低公共祖先</a> <a href="/tags/LCA/" style="font-size: 15px;">LCA</a> <a href="/tags/%E5%A0%86%E7%9A%84%E5%88%A4%E5%AE%9A/" style="font-size: 15px;">堆的判定</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86/" style="font-size: 15px;">字符串处理</a> <a href="/tags/PTA/" style="font-size: 15px;">PTA</a> <a href="/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91Prime%E7%AE%97%E6%B3%95/" style="font-size: 15px;">最小生成树Prime算法</a> <a href="/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91Kruskal%E7%AE%97%E6%B3%95/" style="font-size: 15px;">最小生成树Kruskal算法</a> <a href="/tags/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/" style="font-size: 15px;">贪心算法</a> <a href="/tags/%E5%9B%A2%E4%BD%93%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E5%A4%A9%E6%A2%AF%E8%B5%9B/" style="font-size: 15px;">团体程序设计天梯赛</a> <a href="/tags/map/" style="font-size: 15px;">map</a> <a href="/tags/%E5%89%8D%E7%BC%80%E8%A1%A8%E8%BE%BE%E5%BC%8F/" style="font-size: 15px;">前缀表达式</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%A7%A3%E6%9E%90/" style="font-size: 15px;">字符串解析</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/openpyxl/" style="font-size: 15px;">openpyxl</a> <a href="/tags/tkinter/" style="font-size: 15px;">tkinter</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/JUnit/" style="font-size: 15px;">JUnit</a> <a href="/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/" style="font-size: 15px;">单元测试</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/05/06/linux/screen/">Screen 命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/04/22/linux/docker-installation/">Docker 安装与配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/04/18/linux/terminal-proxy/">Linux 终端开启代理</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/03/31/linux/apt-errors/">Ubuntu 中的 apt 错误</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/03/28/linux/switch-intel-nvidia-graphics-card/">How To Switch Between Intel and Nvidia Graphics Card on Ubuntu</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/03/05/linux/ssh-disconnect-solution/">SSH 自动断连问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/02/27/linux/create-systemd-service/">Linux 中创建系统服务</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/01/28/linux/linux-firewall/">Linux 设置防火墙</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/01/17/linux/ssh-jump/">SSH 跳板服务器连接</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/12/01/linux/oh-my-zsh/">Linux终端配置 - ohmyzsh</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://pintia.cn" title="拼题A" target="_blank">拼题A</a><ul></ul><a href="https://kotlinlang.org" title="Kotlin" target="_blank">Kotlin</a><ul></ul><a href="https://www.patest.cn" title="PAT" target="_blank">PAT</a><ul></ul><a href="https://en.cppreference.com" title="C++ Reference" target="_blank">C++ Reference</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">Zhichao's Blog | </a><a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener">浙ICP备18033074号-1</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/viz.js/1.7.1/viz.js"></script><script>String.prototype.replaceAll = function(search, replacement) {
  var target = this;
  return target.split(search).join(replacement);
};

let vizObjects = document.querySelectorAll('.graphviz')

for (let item of vizObjects) {
  let svg = undefined
  try {
    svg = Viz(item.textContent.replaceAll('–', '--'), 'svg')
  } catch(e) {
    svg = `<pre class="error">${e}</pre>`
  }
  item.outerHTML = svg
}</script></div></body></html>